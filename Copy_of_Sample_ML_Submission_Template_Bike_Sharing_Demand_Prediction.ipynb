{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pruthviraj3196/capstone---2---Bike-Sharing-Demand-Prediction/blob/main/Copy_of_Sample_ML_Submission_Template_Bike_Sharing_Demand_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name              -Pruthviraj Gopinath Barbole**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing has become a popular mode of transportation in urban areas around the world. Bike sharing systems allow users to rent bikes from a network of stations located throughout the city, and to return the bikes to any station in the network. These systems are often used for short, one-way trips, and can be a convenient and affordable alternative to driving or public transportation.\n",
        "\n",
        "In this project, we explore the problem of bike sharing demand prediction using a publicly available dataset from the UCI Machine Learning Repository. The dataset contains hourly bike rental counts for a bike sharing system in Washington D.C. over a period of two years, along with a variety of weather and seasonal features.\n",
        "\n",
        "Our goal is to build a machine learning model that can accurately predict the number of bikes that will be rented at each hour of the day, based on the available features. To do this, we first explore and preprocess the data, including handling missing values, encoding categorical variables, and scaling the numeric features.\n",
        "\n",
        "We then train several different machine learning models, including linear regression, decision trees, random forests, and gradient boosting, and evaluate their performance using a variety of metrics, including mean absolute error, mean squared error, and R-squared. We also use feature importance analysis to identify which features are most important for predicting bike demand.\n",
        "\n",
        "Finally, we discuss the implications of our findings for bike sharing companies and city planners. Our results suggest that machine learning can be a powerful tool for predicting bike demand and optimizing bike allocation, which could help to improve the efficiency and sustainability of bike sharing systems. However, there are also important ethical and privacy considerations to be addressed, such as ensuring that the data used in these models is collected and used in a responsible and transparent manner.\n",
        "\n",
        "Overall, this project highlights the potential of machine learning to address real-world challenges in transportation and urban planning, and underscores the importance of careful data analysis and model evaluation in developing effective solutions."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes..**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RspAP4JbqFHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGGvyuSin18c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1gPCY1mSXtj"
      },
      "outputs": [],
      "source": [
        "df_bike  = pd.read_csv('/content/drive/MyDrive/data/bike data/SeoulBikeData.csv',encoding='unicode_escape')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3meWWQ6SXoD"
      },
      "outputs": [],
      "source": [
        "df_bike.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFzHUJiiSXnF"
      },
      "outputs": [],
      "source": [
        "df_bike.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.info()"
      ],
      "metadata": {
        "id": "R6bmNudssXWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc5-vQW6SXYg"
      },
      "outputs": [],
      "source": [
        "df_bike.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above data after count the missing and duplicate value we came to know that there are no missing and duplicate value present"
      ],
      "metadata": {
        "id": "7MKQdFTNA6r3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3pyA4UISXcr"
      },
      "outputs": [],
      "source": [
        "df_bike.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "HR2reLMg25YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df_bike.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vtSsBVvRytzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 This Dataset contains 8760 lines and 14 columns.\n",
        "\n",
        "2 Three categorical features ‘Seasons’, ‘Holiday’, & ‘Functioning Day’. \n",
        "\n",
        "3 One Datetime features ‘Date’.\n",
        "\n",
        "4 We have some numerical type variables such as temperature, humidity, wind, visibility, dew point temp, solar radiation, rainfall, snowfall which tells the environment conditions at that particular hour of the day."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-sToM76SXi5"
      },
      "outputs": [],
      "source": [
        "df_bike.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.describe()"
      ],
      "metadata": {
        "id": "Ypg0wcUcsqky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date - year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of the day\n",
        "\n",
        "Temperature-Temperature in Celsius\n",
        "\n",
        "Humidity - Humidity in the air in %, type : int\n",
        "\n",
        "Windspeed -peed of the wind in m/s, type : Float \n",
        "\n",
        "*Visibility(10m)* : Visibility in m, type : int\n",
        "\n",
        "*Dew point temperature* : Temperature at the beginning of the day in celsius, type : int\n",
        "\n",
        "*Solar Radiation(MJ/m2)* : Sun contribution, type : Float\n",
        "\n",
        "*Rainfall(mm)* : Amount of raining in mm, type : float\n",
        "\n",
        "*Snowfall(cm)* : Amount of snowing in cm, type : float\n",
        "\n",
        "*Seasons* : Season of the year, type : str (Four types of season's present in data)\n",
        "\n",
        "*Holiday* : If the day is Holiday or not, type : str\n",
        "\n",
        "*Functional Day* : If the day is a Functioning Day or not, type : str"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Check Unique Values for each variable.\n",
        "for i in df_bike.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df_bike[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Some of the columns name in the dataset are too large and clumsy so we change the the into some simple name, and it don't affect our end results.\n",
        "\n"
      ],
      "metadata": {
        "id": "VJGURvjgBHc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming Columns\n",
        "df_bike.rename(columns={'Date': 'date', 'Rented Bike Count': 'bike_count', 'Hour': 'hour',\n",
        "                   'Temperature(°C)': 'temp', 'Humidity(%)': 'humidity', 'Wind speed (m/s)': 'wind',\n",
        "                   'Visibility (10m)': 'visibility', 'Dew point temperature(°C)': 'dew_temp',\n",
        "                   'Solar Radiation (MJ/m2)': 'sunlight', 'Rainfall(mm)': 'rain', 'Snowfall (cm)': 'snow',\n",
        "                   'Seasons': 'season', 'Holiday': 'holiday', 'Functioning Day': 'functioning_day'}, inplace=True)"
      ],
      "metadata": {
        "id": "Hy7ZtzNTvHgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "lxb6HTjPtAbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.info()"
      ],
      "metadata": {
        "id": "9TLtZWFTvHUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLfwPGDbyLov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding numerical_features from the dataset\n",
        "numerical_features = df_bike.describe().columns\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "vsI8d3cyWTuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(10,7))\n",
        "  sns.distplot(x=df_bike[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cy0eCQYCWTrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\\\n",
        " subplot is use to check the column skewness which meansthe column is right skewed or left skewed."
      ],
      "metadata": {
        "id": "k_YlDafwvC3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\\\n",
        "In this plots we observe that some of our columns is right skewed and some are left skewed we have to remember this things when we apply algorithms\\\n",
        "Right skewed columns are :- Rented Bike Count (Its also our Dependent variable), Wind speed (m/s), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm),\\\n",
        "Left skewed columns are :- Visibility (10m), Dew point temperature(°C)"
      ],
      "metadata": {
        "id": "CAsSSmPQvI4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\\\n",
        "right skewed and left skewed columns contains negative impact on our analysis."
      ],
      "metadata": {
        "id": "U-jZMWzTvR6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the box plot for numerical_features\n",
        "for feature in numerical_features[1:]:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.boxplot(x = df_bike[feature])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1q0RdpowWTpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ArvQ13RpuOsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Boxplot is use to detect the outlier of the columns thats why we use box plot."
      ],
      "metadata": {
        "id": "K-4KtLMeuSMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2TnIiKWeuWin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the columns that contain outliers are Rainfall, Snowfall, Windspeed and Solar Radiation. we can remove outlier in the future when we do feature engineering."
      ],
      "metadata": {
        "id": "84neBF0lucXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "UbXWF6V7uRLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a regression plot to find this correlation. This also finds if the independent variable has a linear relationship with the dependent variable, which is an assumption that has to be satisfied for models like linear regression."
      ],
      "metadata": {
        "id": "hybJbWaEuqCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers"
      ],
      "metadata": {
        "id": "V-hJTEcg2h47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers by Using IQR method:\n",
        "q1, q3, median = df_bike.bike_count.quantile([0.25,0.75,0.5])\n",
        "lower_limit = q1 - 1.5*(q3-q1)\n",
        "upper_limit = q3 + 1.5*(q3-q1)\n",
        "df_bike['bike_count'] = np.where(df_bike['bike_count'] > upper_limit, median,np.where(\n",
        "                            df_bike['bike_count'] < lower_limit,median,df_bike['bike_count']))\n",
        "\n",
        "# Removing outliers by Capping:\n",
        "for col in ['wind','sunlight','rain','snow']:\n",
        "  upper_limit = df_bike[col].quantile(0.99)\n",
        "  df_bike[col] = np.where(df_bike[col] > upper_limit, upper_limit, df_bike[col])"
      ],
      "metadata": {
        "id": "1TaY_96hyguI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning and Manipulating dataset"
      ],
      "metadata": {
        "id": "FZYgmZYA3R3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3LwpRGjSXXj"
      },
      "outputs": [],
      "source": [
        "#convert the \"date\" column into 3 different columns i.e \"year\",\"month\",\"day\"\n",
        "import datetime as dt\n",
        "df_bike['date'] = df_bike['date'].apply(lambda x: dt.datetime.strptime(x,\"%d/%m/%Y\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVD_2VEoSXT2"
      },
      "outputs": [],
      "source": [
        "df_bike['year'] = df_bike['date'].dt.year\n",
        "df_bike['month'] = df_bike['date'].dt.month\n",
        "\n",
        "df_bike['day'] = df_bike['date'].dt.day_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vleWOV17SXSy"
      },
      "outputs": [],
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "df_bike['week']=df_bike['day'].apply(lambda x : \"weekend\" if x=='Saturday' or x=='Sunday' else \"weekday\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9FI3INwSXOP"
      },
      "outputs": [],
      "source": [
        "# checking no of years\n",
        "df_bike['week'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "xD5hk-nwv-4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.columns"
      ],
      "metadata": {
        "id": "ztngz7gVv-1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike=df_bike.drop(columns=['date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "pqBuMeFMv-yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "rpfxQAl2v-wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike['timeshift'] = df_bike['hour'].apply(lambda x: 'night' if 0<=x<=6 else ('day' if 7<=x<=16 else 'evening'))"
      ],
      "metadata": {
        "id": "sGamqRWZv-tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "4SZ0zHKvzf92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "dk4ijnS7qWZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\". The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november. The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column. "
      ],
      "metadata": {
        "id": "MEjhk8XcqZ7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking AS PER  categorical feature\n",
        "cat_fea =df_bike.describe(include=['object']).columns\n",
        "     "
      ],
      "metadata": {
        "id": "6_bIxVp32tQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_fea"
      ],
      "metadata": {
        "id": "Fwhl2tNe2tQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a bar plot for each categorical feature count  \n",
        "\n",
        "for col in cat_fea:\n",
        "    counts =df_bike[col].value_counts().sort_index()\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    counts.plot.bar(ax = ax, color='red')\n",
        "    ax.set_title(col + ' counts')\n",
        "    ax.set_xlabel(col) \n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9_azjWa2NHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "JEO_VrfOqrcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent."
      ],
      "metadata": {
        "id": "zJQ0-U-hq9zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "qgAoqrUmrCJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - seasons- From the above bar plot it is clear that in summer season Rented bike count is very high and in Winter season Rented bike count is very low.\\\n",
        "2 - Holiday - As we can see that the majority of the bikes rented are on days which are considered as No Holiday. \\\n",
        "3-function Day - it is clear from above plot rented bikes are rented on functioning day.\\\n",
        "4- Timeshift - as compair to 3 stages of days during day time bike requirement as seen to be more and during evening and night its are same moderate \n"
      ],
      "metadata": {
        "id": "B1NErYhZrHRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "zCPV8JGGrbdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - seasons- winter is coldest season of the year thats why customers prefer taxi or cars over the rented bike thats why winter season is negative growth.\\\n",
        "2 - Holiday -Positive growth is clearly seen in the no holiday days because on no holiday days office or work is going on where on holiday days negative growth is occur because on holiday days office or work is shut down.\\\n",
        "3-function Day - rented bikes have huge loss on no functioning day.\\\n",
        "4 - Timeshift - As night time there less number of people  as compair to day time "
      ],
      "metadata": {
        "id": "6iSZYpKSrlKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression plot"
      ],
      "metadata": {
        "id": "onw0UeiH68vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_variable =  ['temp','dew_temp', 'humidity', 'wind', 'visibility', 'sunlight', 'rain', 'snow']"
      ],
      "metadata": {
        "id": "q9WvulcQ58jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regression plot between Rented_Bike_Count and other variables\n",
        "for i in continuous_variable:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=df_bike[i],y=df_bike['bike_count'],scatter_kws={\"color\": 'green'}, line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "HUwhaKq13pSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\\\n",
        " We will use a regression plot to find this correlation. This also finds if the independent variable has a linear relationship with the dependent variable, which is an assumption that has to be satisfied for models like linear regression."
      ],
      "metadata": {
        "id": "DhX5B2iuvsov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\\\n",
        "This regression plots shows that some of our features are positive linear and some are negative linear in relation to our target variable."
      ],
      "metadata": {
        "id": "q3ktl7FYv2LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Categorical Encoding"
      ],
      "metadata": {
        "id": "USEObRSn3XHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take care of the categorical features\n",
        "categorical_features = [i for i in df_bike.columns if i not in df_bike.describe().columns]\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "6mdZr4Le1MVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking unique value with their counts in categorical features\n",
        "for col in categorical_features:\n",
        "  print(df_bike[col].value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "RuVtDEq61iln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a label encoder based on above data\n",
        "encoder = {'holiday':{'Holiday':1, 'No Holiday':0},'week':{'weekday':1, 'weekend':0},'functioning_day':{'Yes':1, 'No': 0},\n",
        "          'timeshift': {'night':0, 'day':1, 'evening':2}}"
      ],
      "metadata": {
        "id": "NeLsxGVN1ijE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "oB795Fk21igt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "df_bike = df_bike.replace(encoder)\n",
        "\n",
        "# One Hot Encoding\n",
        "df_bike = pd.get_dummies(df_bike, columns=[\"season\"], prefix='', prefix_sep='')"
      ],
      "metadata": {
        "id": "kCHZ3D2u4Ss8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bike.head()"
      ],
      "metadata": {
        "id": "aIOaM8we1ieE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "qNPbhroz3-sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Label Encoding technique for bike data, holiday , week , function day  & time shift column . As we have seen during analysis, that categorical columns are very very important Machine learning models can only work with numerical values and therefore important categorical columns have to converted/encoded into numerical variables. This process is known as Feature Encoding "
      ],
      "metadata": {
        "id": "E8v7BWvu4HOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "independent_variables = [i for i in df_bike.columns if i not in ['bike_count']]\n",
        "\n",
        "# Checking Linearity\n",
        "plt.figure(figsize=(18,18))\n",
        "for n,column in enumerate(independent_variables):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.regplot(data = df_bike, x = column, y ='bike_count',line_kws={\"color\": \"red\"})\n",
        "  plt.title(f'Bike_Count v/s {column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "9futUvdt4pCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hour:\n",
        "1)There is sudden peak between 6/7AM to 10 AM. Office time,College and going time could be the reason for this sudden peak.\n",
        "2) Again there is peak between 10 AM to 7 PM. may be its office leaving time for the above people.\n",
        "3) We can say that,from morning 7 AM to Evening 7 PM we have good Bike Rent Count. and from 7 PM to 7 AM Bike Rent count starts declining.\n",
        "Temperature:\n",
        "1) For decrease in temperature below 0 deg celicus the bike rent count is significantly decreased because may be people dont want to ride bike in such cold temperature.\n",
        "2) But for normal temperature the Bike rent count is very high.\n",
        "humidity\n",
        "1) Here its seems like humidty is inversely proportional to bike rent count. As humdity percentage is increasing there is decrease in bike rent count.\n",
        "Wind Speed:\n",
        "1) upto wind speed 4 m/s there is good bike rent count.\n",
        "Visibility\n",
        "1) It's very obivious that as visibilty increases the bike rent count also increases. Nobody would prefere to ride in low visibilty.\n",
        "Dew Point Temperature\n",
        "1)It's again the same case as of temperature. As dew temperature goes below 0 deg celcius there is less bike rent count. It looks like Dew Point temperature and Temperature columns have strong colinarity.\n",
        "Solar radiation\n",
        "1)Here the amount of rented bikes is huge, when there is solar radiation.\n",
        "Rainfall And snowfall\n",
        "1) Its very obivious that people usually do not like ride bikes in rain and snowfall."
      ],
      "metadata": {
        "id": "u4meyYMxKG_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking skewness of the dependend variable\n",
        "print(f'Skewness of original data : {df_bike.bike_count.skew()}')\n",
        "print(f'Skewness after log transformation : {np.log(df_bike.bike_count).skew()}')\n",
        "print(f'Skewness after transformation : {np.sqrt(df_bike.bike_count).skew()}')"
      ],
      "metadata": {
        "id": "VzkiDNL24o_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "iYfZAwG77dGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Since Sqrt Transformation gives skewness between -0.5 and 0.5 indicates that the distribution is fairly symmetrical we will use it\n",
        "plt.figure(figsize=(9,4))\n",
        "plot = plt.subplot(1,2,1)\n",
        "sns.distplot(df_bike['bike_count']).set_title('Bike_Count Before Transformation',weight='bold')\n",
        "plot = plt.subplot(1,2,2)\n",
        "sns.distplot(np.sqrt(df_bike['bike_count'])).set_title('Bike_Count After Transformation',weight='bold')\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "7aWNELS38Jey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph shows that Rented Bike Count has moderate right skewness. \n",
        "\n",
        "Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform Square root operation to make it normal.\n",
        "\n",
        "After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution. "
      ],
      "metadata": {
        "id": "zQ6G0vbMDrCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Multicolinearity"
      ],
      "metadata": {
        "id": "xcjPq_7Mw-Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ],
      "metadata": {
        "id": "0D6SU8De8Zcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to calculate Variance Inflation factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif.sort_values(by='VIF',ascending=False).reset_index(drop=True))"
      ],
      "metadata": {
        "id": "lyRiE1lZ4o3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. VIF score of an independent variable represents how well the variable is explained by other independent variables."
      ],
      "metadata": {
        "id": "IC7UOR1sxGni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking corelations\n",
        "plt.figure(figsize=(18,9))\n",
        "plot=sns.heatmap(abs(df_bike.corr()), annot=True, cmap='coolwarm')\n",
        "plot.set_xticklabels(plot.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K2l2CNRvxLjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see some highly correlated features. Lets treat them by excluding them from dataset and checking the variance inflation factors."
      ],
      "metadata": {
        "id": "FVSo8vqRxSua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking VIF for each variable\n",
        "independent_variables = [i for i in df_bike.columns if i not in ['bike_count']]\n",
        "calc_vif(df_bike[independent_variables])"
      ],
      "metadata": {
        "id": "bw-B9cylxYXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Summer and Winter can also be classified on the basis of temperature and we already have that feature present. Even if we drop these features the useful information will not be lost. So lets drop them."
      ],
      "metadata": {
        "id": "MzxItOx7xdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summer and Winter are highly correlated with temperature. Hence removing them\n",
        "independent_variables = [i for i in df_bike.columns if i not in ['bike_count','Winter','Summer','dew_temp','hour','humidity']]\n",
        "calc_vif(df_bike[independent_variables])"
      ],
      "metadata": {
        "id": "VbBUfitExeO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the dataset\n",
        "dataset = df_bike[independent_variables + ['bike_count']]\n",
        "\n",
        "#checking corelations\n",
        "plt.figure(figsize=(18,9))\n",
        "plot=sns.heatmap(abs(dataset.corr()), annot=True, cmap='coolwarm')\n",
        "plot.set_xticklabels(plot.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q_D3I6JTxjiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\\\n",
        "A correlation heatmap is a good chart choice to visualize the relationships between multiple variables in a dataset. It shows the correlation coefficients between each pair of variables as a color-coded matrix, where the intensity of the color represents the strength of the correlation. By using a correlation heatmap, we can easily identify the variables that have a strong positive or negative correlation with each other, which can help in feature selection and modeling. Therefore, it is a good choice for exploring the relationships between different variables in the Bike dataset."
      ],
      "metadata": {
        "id": "cCt-EWY2wbFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Linearity of the new dataset"
      ],
      "metadata": {
        "id": "UL6q28GLylaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regression plot between Rented_Bike_Count and other variables\n",
        "for i in dataset:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=df_bike[i],y=df_bike['bike_count'],scatter_kws={\"color\": 'green'}, line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "916gzJqD0gAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking top 5 rows of the cleaned dataset\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "DRnY1GCP12LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all feature selection methods have you used and why?"
      ],
      "metadata": {
        "id": "7dAuaJyUuntv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I have used the Variance Inflation Factor (VIF) method to check for feature selection. The VIF score indicates how much the variance of a regression coefficient is increased due to multicollinearity in the data. High VIF values indicate high multicollinearity among the independent variables, which may lead to inaccurate and unstable estimates of the regression coefficients. Therefore, it is essential to remove highly correlated variables or combine them to reduce their VIF score to an acceptable level. Hence, I have used the VIF method to identify highly correlated variables in the data set and then remove or combine them to reduce multicollinearity."
      ],
      "metadata": {
        "id": "NX_lb88ZupDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which all features you found important and why?"
      ],
      "metadata": {
        "id": "aSsdpRDvusab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the VIF score, it seems like all the features have VIF values lower than the threshold value of 10, indicating that there is no multicollinearity issue between the features.\n",
        "\n",
        "But we can see that temperature and dew point temperature has VIF value 33.659374 and 17.264367 respectively indicating multicollinearity. This means that these two variables are highly correlated with other independent variables in the dataset. In such cases, it is generally recommended to remove one of the highly correlated variables to reduce the impact of multicollinearity on the regression model. However, before making a final decision, it is important to consider the domain knowledge and the impact of each variable on the dependent variable. In this case, temperature and dew point temperature are both important factors that can influence bike rental counts. Therefore, it may be worthwhile to keep both variables in the model and explore other methods to handle multicollinearity, such as regularization techniques.\n",
        "\n",
        "Hence, all features can be considered as important for modeling."
      ],
      "metadata": {
        "id": "qtwBvhQtuvUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Independent and Dependent & Data Splitting"
      ],
      "metadata": {
        "id": "Eg-1oLF2vfWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV8MU6aIoOKo"
      },
      "outputs": [],
      "source": [
        "# Data for independent and dependent set\n",
        "X=dataset.drop(['bike_count'],axis=1)\n",
        "y=dataset.loc[:,'bike_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-qaUAjuoOCg"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of independent and dependent dataset\n",
        "print(X.shape)\n",
        "print(y.shape)    "
      ],
      "metadata": {
        "id": "lyLXWdB9rvON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spiliting the data using the Train Test Split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)    "
      ],
      "metadata": {
        "id": "xBqJNngKruxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the size of training and testing data\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)     "
      ],
      "metadata": {
        "id": "E4SMyY2Kr9K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 . What you have done and what insight you have got?"
      ],
      "metadata": {
        "id": "6bQ0oAkGwcRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date and Dew point temperature has been dropped from our dataset fro modelling. because no need to keep date and due to multicolinearity dew point has been dropped.\\\n",
        "Then I have splitted the data set into 80:20 ratio for train test analysis.\\\n",
        "The training dataset contains 7008 records with 13 features and the corresponding target variable, and the testing dataset contains 1752 records with 13 features and the corresponding target variable."
      ],
      "metadata": {
        "id": "JlRwYyBEwdFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - What data splitting ratio have you used and why?\\\n",
        "Data splitting is when data is divided into two or more subsets. Typically, with a two-part split, one part is used to evaluate or test the data and the other to train the model. Data splitting is an important aspect of data science, particularly for creating models based on data."
      ],
      "metadata": {
        "id": "fo5uniIPxbiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scaling"
      ],
      "metadata": {
        "id": "7zISWVxRwXYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we rescaling our data\n",
        "scalar=MinMaxScaler()\n",
        "X_train=scalar.fit_transform(X_train)\n",
        "X_test=scalar.transform(X_test)   "
      ],
      "metadata": {
        "id": "-wCJPuFMsCKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "CW8YjvwUsgSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "TuFXcoqRxE3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used MinMaxScaler to scale the data.\n",
        "\n",
        "MinMaxScaler scales the data between 0 and 1. This is particularly useful for neural networks, SVMs, and algorithms that require input data to be normalized to a uniform range. It preserves the shape of the original distribution and does not distort the relative distances between data points.\n",
        "\n",
        "In addition, MinMaxScaler is less prone to the influence of outliers compared to other scaling methods."
      ],
      "metadata": {
        "id": "t6enVj49xFjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models can be described as programs that are trained to find patterns or trends within data and predict the result for new data.\n",
        "\n",
        "In this project we are dealing with a regression problem, therefore we will be using regression models. Some popular examples are Linear Regression and polynomial regression.\n",
        "\n",
        "In this project we will be include the following models:\n",
        "\n",
        "1.Linear regression.\n",
        "\n",
        "2.Lasso regression (Linear regression with L1 regularization).\n",
        "\n",
        "3.Decision Tree\n",
        "\n",
        "4.Random forest regression."
      ],
      "metadata": {
        "id": "01jhAepixwZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ],
      "metadata": {
        "id": "w1_KMXZq1OVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Multiple Linear Regression to the Training set\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "AwY9mSIm1M4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting on Training set\n",
        "y_pred_train = regressor.predict(X_train)"
      ],
      "metadata": {
        "id": "MiVL0ceY1jwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "AcEKn6Mk1lq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Performance\n",
        "#Mean Squared error\n",
        "MSE  = mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "#Root Mean Squared Error\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "#R2 score\n",
        "r2 = r2_score((y_train), (y_pred_train))\n",
        "print(\"R2 :\" ,r2)\n",
        "\n",
        "#Adjusted R2 score\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score((y_train), (y_pred_train)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))"
      ],
      "metadata": {
        "id": "CLy3hA5S1ssY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Performance\n",
        "#Mean Squared Error\n",
        "MSE  = mean_squared_error((y_test), (y_pred))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "#Root Mean Squared Eror\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "#R2 score\n",
        "r2 = r2_score((y_test), (y_pred))\n",
        "print(\"R2 :\" ,r2)\n",
        "\n",
        "#Adjusted R2 score\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score((y_test), (y_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "mvMWf3S6183Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe of actual vs predicted values\n",
        "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})"
      ],
      "metadata": {
        "id": "5iQs7QRk2SGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random sampling from predictions dataframe and plotting\n",
        "predictions.sample(50).plot(kind='bar',figsize=(14,8))\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tO32DqLt2VSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** \\\n",
        "From the above output, we can see that the R-squared value is 0.5573 which means that approximately 54% of the variance in the target variable is explained by the independent variables in our model. The adjusted R-squared value is 0.5540 which is similar to the R-squared value.\n",
        "\n",
        "The Root Mean Squared Error (RMSE) is 390.6229 which means that the average difference between the predicted bike count and the actual bike count is 880.83. The Mean Squared Error (MSE) is 152586.2585 which is the average of the squared differences between the predicted bike count and the actual bike count.\n",
        "\n",
        "we have to make our model more complex for better discretion or move to tree and ensembling algorithm for better results"
      ],
      "metadata": {
        "id": "yEmhU2Ey3Duw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ubLIZqvqA5uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Linear Regression. Linear Regression is a supervised learning algorithm used for predicting a continuous target variable based on one or more input features.\n",
        "\n",
        "Now, let's analyze the performance of the Linear Regression model using the provided evaluation metric score chart:\n",
        "\n",
        " the provided evaluation metric score chart indicates that the Linear Regression model has an R-squared value of 0.5573, suggesting a moderate level of prediction accuracy. The adjusted R-squared value of 0.5540 indicates a similar level of accuracy, considering the model's complexity. The RMSE value of 390.6229 indicates the average prediction error, and the MSE value of 152586.2585 provides a measure of the squared error."
      ],
      "metadata": {
        "id": "ZQMGAcL4A60q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_1  = Lasso(alpha=0.001 , max_iter= 3000)\n",
        "lasso_1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "IwYNfPCw6M_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy of lasso regression model\n",
        "lasso_1.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "a5ZKmph18S_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_1.coef_"
      ],
      "metadata": {
        "id": "FZMqWQNF8V5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction of test data\n",
        "y_pred_lasso_1 = lasso_1.predict(X_test)"
      ],
      "metadata": {
        "id": "APNfVsyN8aJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the Evaluation Metrics\n",
        "MSE  = mean_squared_error(y_test, y_pred_lasso_1)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(y_test,y_pred_lasso_1)\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test, y_pred_lasso_1))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "mY8hT-0n9XhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual price vs predicted  for lasso regression ploting\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot((y_pred_lasso_1)**2)\n",
        "plt.plot(np.array((y_test)**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "61QV5wDO9HNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** \\\n",
        "From the above output, we can see that the R-squared value is 0.5573 which means that approximately 54% of the variance in the target variable is explained by the independent variables in our model. The adjusted R-squared value is 0.5540 which is similar to the R-squared value.\n",
        "\n",
        "The Root Mean Squared Error (RMSE) is 390.6229 which means that the average difference between the predicted bike count and the actual bike count is 880.83. The Mean Squared Error (MSE) is 152586.2103 which is the average of the squared differences between the predicted bike count and the actual bike count.\n",
        "\n",
        "we have to make our model more complex for better discretion or move to tree and ensembling algorithm for better results"
      ],
      "metadata": {
        "id": "JDKtgCYh9r3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "3nbNxl3rB-kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Lasso Regression. Lasso Regression is a linear regression technique that includes a regularization term to shrink the coefficients of the less important features to zero, effectively performing feature selection.\n",
        "\n",
        "Now, let's analyze the performance of the Lasso Regression model using the provided evaluation metric score chart:\n",
        "\n",
        "The provided evaluation metric score chart indicates that the Lasso Regression model has an R-squared value of 0.5573, suggesting a moderate level of prediction accuracy. The adjusted R-squared value of 0.5540 indicates a similar level of accuracy, considering the model's complexity. The RMSE value of 390.6229 indicates the average prediction error, and the MSE value of 152586.2103 provides a measure of the squared error. It's worth noting that the performance metrics for Lasso Regression are identical to those of Linear Regression in this case, which might indicate that the regularization term didn't have a significant impact on feature selection."
      ],
      "metadata": {
        "id": "Mgo9MXiNB_dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "0fo3hkmh_VgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For decision tree we use the standard scalar\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "X_train=scalar.fit_transform(X_train)\n",
        "X_test=scalar.transform(X_test)"
      ],
      "metadata": {
        "id": "ZpKIXt3W_fba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Decision Tree Algorithm\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor=DecisionTreeRegressor(criterion='friedman_mse', max_leaf_nodes=9, max_depth=5)\n",
        "regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "gBf3OYhnEGYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the traning\n",
        "regressor.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "652oBBxzEKFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecting the result on the test data\n",
        "y_pred=regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "ITuPHWXGENqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcuting the R_squared for test data\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "C1kkT6yXERGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Grid Search for Decision Tree\n",
        "param = {'max_depth' : [1,4,5,6,7,10,15,20,8], 'max_leaf_nodes':[5,10,20,25,30,40,45]}\n",
        "\n",
        "decision_tree=DecisionTreeRegressor()\n",
        "\n",
        "gridSearch_decisionTree=GridSearchCV(decision_tree,param,scoring='r2',cv=5)\n",
        "gridSearch_decisionTree.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "WQ_fzkkyEWpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best params for decision tree\n",
        "gridSearch_decisionTree.best_params_"
      ],
      "metadata": {
        "id": "feYs2n9FEcoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score and optimal paramters\n",
        "print('The best hyperparameter for Decision Tree :',gridSearch_decisionTree.best_params_)\n",
        "print('The best score:',gridSearch_decisionTree.best_score_)"
      ],
      "metadata": {
        "id": "qWFGBkoBEhay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal Model for decision tree\n",
        "optimal_DecisionTree=gridSearch_decisionTree.best_estimator_"
      ],
      "metadata": {
        "id": "8vDwNabfEkel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the Output value\n",
        "y_pred_dt=optimal_DecisionTree.predict(X_test)"
      ],
      "metadata": {
        "id": "3sLaXL99Enmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning score\n",
        "optimal_DecisionTree.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "RW8jmW1MEopx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the Evaluation Metrics\n",
        "MSE  = mean_squared_error(y_test, y_pred_dt)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(y_test,y_pred_dt)\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test, y_pred_dt))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "y7XplO7WErn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot figure for Actual and Predicted test value\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_dt**2)[:100])\n",
        "plt.plot((np.array(y_test**2)[:100]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Number of Test Data', fontsize=16)\n",
        "plt.title('Decision Tree',fontsize= 20, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NcuDwJi5EyMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** \\\n",
        "From the above output, we can see that the R-squared value is 0.6969 which means that approximately 69% of the variance in the target variable is explained by the independent variables in our model. The adjusted R-squared value is 0.6946 which is similar to the R-squared value.\n",
        "\n",
        "The Root Mean Squared Error (RMSE) is 323.2012 which means that the average difference between the predicted bike count and the actual bike count is 880.83. The Mean Squared Error (MSE) is 104459.0230 which is the average of the squared differences between the predicted bike count and the actual bike count.\n",
        "\n",
        "we have to make our model more complex for better discretion or move to tree and ensembling algorithm for better results"
      ],
      "metadata": {
        "id": "gDgzdHsk-Z10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "pN82ITI2CzMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Decision Tree. Decision Tree is a non-parametric supervised learning algorithm that creates a tree-like model of decisions and their possible consequences.\n",
        "\n",
        "Now, let's analyze the performance of the Decision Tree model using the provided evaluation metric score chart:\n",
        "\n",
        "The provided evaluation metric score chart indicates that the Decision Tree model has an R-squared value of 0.6969, suggesting a relatively good level of prediction accuracy. The adjusted R-squared value of 0.6946 indicates a similar level of accuracy, considering the complexity of the model and the number of input features. The RMSE value of 323.2012 indicates the average prediction error, and the MSE value of 104459.0230 provides a measure of the squared error. Overall, the Decision Tree model seems to perform well based on the provided evaluation metrics."
      ],
      "metadata": {
        "id": "V7TzpiGjC0X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "Q1oHrkuDLXXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "hmB4H0pMLhD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Providing the range of values for the hyperparameter, so it can be used for gridsearch\n",
        "n_estimators=[75,100,125]\n",
        "\n",
        "# Max Depth of the tree\n",
        "max_depth=[4,6,8]\n",
        "\n",
        "# Minimum number of samples requires for the spilting of a node\n",
        "min_samples_split=[50, 70, 90, 110]\n",
        "\n",
        "# Minimum number of samples in the leaf node\n",
        "min_samples_leaf=[40,50]   # To avoid the overfitting of data\n",
        "\n",
        "# Hyperparameter Grip\n",
        "grid_dict={'n_estimators':n_estimators,\n",
        "            'max_depth':max_depth,\n",
        "            'min_samples_split':min_samples_split,\n",
        "            'min_samples_leaf':min_samples_leaf}\n",
        "     "
      ],
      "metadata": {
        "id": "Au0zLCrjWGtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crearing an instance of the random forest\n",
        "rf_model=RandomForestRegressor()\n",
        "\n",
        "# Perform the gridsearch\n",
        "rf_grid=GridSearchCV(estimator=rf_model, param_grid=grid_dict, scoring='r2',verbose=0, cv=5)\n",
        "\n",
        "rf_grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "SBSTOOolMcgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best parameters for the RandomForestRegressor\n",
        "rf_grid.best_params_"
      ],
      "metadata": {
        "id": "R9bBufeVMiBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning Score \n",
        "print('The best score:',rf_grid.best_score_)"
      ],
      "metadata": {
        "id": "wdMUISG7N2sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal Model\n",
        "rf_optimal_model=rf_grid.best_estimator_"
      ],
      "metadata": {
        "id": "UBWeNUajN7Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test data\n",
        "y_pred_rf=rf_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "ht6YNRy6N-bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Score\n",
        "rf_optimal_model.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "l8YAKvRJOA2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the Evaluation Metrics\n",
        "MSE  = mean_squared_error(y_test, y_pred_rf)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(y_test,y_pred_rf)\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(y_test, y_pred_rf))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "kJNSokMsOEi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting actual and predicted values and the feature importances:\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.plot((y_pred_rf)[:500])\n",
        "plt.plot((np.array(y_test)[:500]))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.title('Actual and Predicted Bike Counts')"
      ],
      "metadata": {
        "id": "axeMHSE_OHez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** \\\n",
        "From the above output, we can see that the R-squared value is 0.6948 which means that approximately 54% of the variance in the target variable is explained by the independent variables in our model. The adjusted R-squared value is 0.6925 which is similar to the R-squared value.\n",
        "\n",
        "The Root Mean Squared Error (RMSE) is 324.3009 which means that the average difference between the predicted bike count and the actual bike count is 880.83. The Mean Squared Error (MSE) is 105171.1026 which is the average of the squared differences between the predicted bike count and the actual bike count.\n",
        "\n",
        "we have to make our model more complex for better discretion or move to tree and ensembling algorithm for better results"
      ],
      "metadata": {
        "id": "2IahxVTu-3wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "kDqhHh6yDkMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Random Forest. Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. It improves upon the individual decision tree's performance by reducing overfitting and increasing prediction accuracy.\n",
        "\n",
        "Now, let's analyze the performance of the Random Forest model using the provided evaluation metric score chart:\n",
        "\n",
        "The provided evaluation metric score chart indicates that the Random Forest model has an R-squared value of 0.6948, suggesting a reasonably good level of prediction accuracy. The adjusted R-squared value of 0.6925 indicates a similar level of accuracy, considering the complexity of the model and the number of input features. The RMSE value of 324.3009 indicates the average prediction error, and the MSE value of 105171.1026 provides a measure of the squared error. Overall, the Random Forest model seems to perform well based on the provided evaluation metrics."
      ],
      "metadata": {
        "id": "YpRG3V0RDnUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The temperature, hours, and solar radiation features were found to be more relevant for the bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "- Other factors such as rainfall and snowfall also have an impact on the requirement of bikes for rent. Because in heavy rainfall and snowfall bike riding sometime becomes dangerous.\n",
        "\n",
        "- As we have analyzed that the rental bike demands are high in the day time. So bikes should be available at that time to fulfill the bike demands.\n",
        "\n",
        "- In the Functioning Day column, If there is no Functioning Day then there is no demand\n",
        "\n",
        "- As we have analyzed the various features, we have seen that people prefer to take bikes on rent when temperature is near about 25 degrees Celcius.\n",
        "\n",
        "- The Bike demand increases with an increase in visibility and decreases with an increase with humidity.\n",
        "\n",
        "Results from ML models:\n",
        "\n",
        "- Decision Tree Regression is the best performing model with an r2 score of 0.6969\n",
        "\n",
        "- Actual vs Prediction visualisation is done for all the 4 models.\n",
        "\n",
        "In conclusion, the tree-based models (Decision Tree and Random Forest) outperform Linear Regression and Lasso Regression in terms of explaining the variance in the target variable (higher R-squared values) and making accurate predictions (lower RMSE and MSE values). Among the tree-based models, the Decision Tree and Random Forest show similar performance, with slightly higher R-squared and adjusted R-squared values for the Decision Tree, but slightly lower RMSE and MSE values for the Random Forest. Therefore, the Decision Tree and Random Forest models may be more suitable for this Model. "
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}